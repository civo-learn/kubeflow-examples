apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "llama3"
  namespace: my-profile
  annotations:
    "sidecar.istio.io/inject": "false"
spec:
  predictor:
    containers:
      - image: ghcr.io/civo-learn/llama3-flask-kf-q8:latest
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: "8"
            memory: "16Gi"
          requests:
            cpu: "8"
            memory: "16Gi"
